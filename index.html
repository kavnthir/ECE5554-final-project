<!DOCTYPE html>

<html lang="en">

    <head>
        <meta charset="UTF-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel ="stylesheet "href="./main.css" type="text/css">
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
        <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
		<!-- Webpage Title -->
        <title>ECE5554 Course Project</title> 
    </head>

    <body>

		<div class="page-header">
			<div class="text-left main" style="margin-top: 30px;">
				<h1> <strong>Infared Object Detection</strong> </h1>
				<h3> <strong> Alex Downey, Aaron Wadhwa, Kavin Thirukonda </strong> </h3> 
				<h4> Fall 2022 ECE 4554/5554 Computer Vision: Course Project </h4>
				<h4> Virginia Tech </h4>
			</div>
		</div>

		<div class="main" style="margin-bottom: 125px;">

			<h2> Abstract </h2>

			<p>Thermal imaging is a popular form of camera recording that extracts more information from a scene than light permits. Being able to accurately identify objects of interest on this footage autonomously in poor lighting and/or exposure is an important requirement for several military and commercial applications. We use the You Only Learn Once (YOLO) framework to develop our object identification systems for infrared (IR) images and RGB images to identify advantages of IR information in low-light conditions. A set of IR and RGB reference images with both training and validation sets was referenced from the Teledyne FLIR Thermal Dataset. Through the development and fine tuning of the object identification model, we were able to obtain annotated images with bounding boxes, object name, and confidence level for several batches of IR and RGB images. We found that the system for IR images produced more accurate predictions and inferences towards images collected in night-time or low-light level conditions compared to the conventional RGB system, which had a greater tendency to struggle when identifying objects of  interest.</p>

			<h2> Teaser Figure </h2>

			<div class="container-fluid text-center" style="padding: 15px;">
				<img src="https://cdn.discordapp.com/attachments/463193265416699915/1046996700427993189/val_batch0_pred.jpg" style="width: 60%">
				<p>Figure 1. Output Predictions for a Batch of IR Images</p>
			</div>

			<h2> Introduction </h2>

			<p>Thermal imaging is the process of developing a heat-mapped image based on the infrared emissions of objects in the surrounding environment. There are various applications for it, however, its primary benefits lie in visualizing an environment with low visibility – whether it be fog, smoke, or any other non-solid barrier. Understanding an environment with poor visibility can allow for improved safety in various applications including railway operation. The goal of this project is to develop a model that can accurately detect common objects including people, bikes, cars, trains, and traffic lights in a given thermal image through image formation, edge detection, and other concepts learned both in class and throughout our courses at Virginia Tech. Previous work in this subject area often references the You Only Learn Once or YOLO model based on convolutional neural networks. In this project, we used similar concepts and applied data  involving both infrared and RGB-based images towards our models, while still building our model with outlines based on this existing framework.  The results of this project provided us with a better understanding of the advantages to the inclusion of an IR based object identification system over a traditional RGB system that can be used in low-light conditions to provide increased accuracy and an additional layer of reliability.</p>

			<h2> Approach </h2>

			<p>The backbone of our approach leveraged a Convolutional Neural Network (CNN) with fully connected layers to identify objects within an IR context.  In the forward stage, an input image was convoluted with a series of feature kernels through successive stages before being flattened into a vector and run through a fully connected network.  Gradient descent was used to tune the weights in the model to minimize error during training.  Research concerning IR object detection strongly favors the “You Only Look Once” (YOLO) architecture to produce bounding boxes and classifications for provided data.  The YOLOv5 framework is one of the more recent, stable versions of YOLO that has a fair degree of literature and documentation concerning its use.  YOLO also hosts several internal tools and features that greatly facilitate the data preparation, training, and evaluation processes, which gave us the flexibility to experiment with different parameters with confidence.  Not only does YOLO identify regions of interest in the data, but it also classifies the identified objects.</p>

			<p>There were also elements of preprocessing that depended on the quality of our data and the format of its annotations.  We developed scripts to make the annotation format compatible with YOLO standards, adjusted images to filter noise, alter size, and experiment with different processing methods, etc.  The dataset we used also contained RGB images as well, which served as the basis for a comparison model to the one created from IR images.  Unfortunately the IR and RGB images were collected independently of each other, or the mappings between corresponding pairs of images were not present, so a direct comparison between images was not possible.  Moreover, while a potential  interest to the team, an ensemble model of both RGB and IR information was infeasible due to this reason.</p>

			<p>Throughout this project, we encountered a variety of hurdles that we had to overcome.  Many of these public-facing, notoriously referenced models and innovations come from research teams or companies that have resources orders of magnitudes greater than ours.  Our team has little access to time, capital, hardware, and software necessary to ingest large quantities of data and produces professional-grade models from it.  In this regard, the FLIR data set contained over ten GB of data, which made it difficult to train/validate over the entire set.  Most open-source frameworks such as  YOLO require the usage of specific parallelization and machine learning libraries that are necessary to run models quickly on local machines.  The lengthy setup and detailed configuration required to properly install these libraries prompted our team to train our models in Google Colab instead, where the initial setup was greatly facilitated.  While Google Colab was the best option for this project and it features some great collaborative tools, it does come with some downsides.  The free version of Colab has limited compute runtime and no GPU support.  Eventually the hosted compute unit will reset regardless of task or training status, erasing current progress and the lack of GPU support further slows model training times as well.  Since models can take upwards of several hours to finish training on the lower end, constant monitoring of the runtime environment was necessary to ensure completion of training and other tasks relevant to the project.  When model creation takes this long, training becomes incredibly tedious and much work was lost to unexpected Colab runtime disconnects.  For this reason, the number of epochs used for training each model were greatly decreased.  The team found success when training models in the order of tens.  Normally, models would be trained in the order of hundreds or even thousands if necessary.  In addition, the usage of Google Colab made it necessary to import data from a local or cloud-based source.  In both cases the large nature of the dataset made it infeasible to upload repeatedly, especially since the team was experimenting with pre-processing steps on the data set at any-given-time.  Moreover, the size of the data set made training times too slow to fit the runtime duration limits put in place by Google Colab.  To mitigate these issues, a subsample of the data was taken for the validation and training sets making it possible to upload data, experiment with processing steps, and train models.  With proper compute and storage options, the team would have been able to use the FLIR data set to our full advantage, which would have improved the robustness of each model.  Overall, the models trained in this project suffer from the lack of resources for a small, student-led, research team.  That having been said, the project contains noteworthy results, which have aided in our understanding of IR-based methodology and serve as a successful proof-of-concept for future researchers to improve upon with greater resources.</p>

			<div class="container-fluid text-center" style="padding: 15px;">
				<img src="https://cdn.discordapp.com/attachments/463193265416699915/1047002484192858193/image.png" style="width: 60%">
				<p>Figure 2. CNN and YOLO architecture</p>
			</div>

			<h2> Experiments and results </h2>

			<p> For this project, we used an open-source thermal dataset for algorithm training by Teledyne FLIR. The dataset contains 26,442 image frames with over 520,000 annotations with 15 objects classifications, such as: person, bike, car, motorcycle, bus, train, truck, traffic light, fire hydrant, street sign, dog, skateboard, stroller, scooter, & other vehicle. Out of these image frames, there are around 9,000 training frames, and 9,000 validation frames as well. The file types include TIFF and JPEG for the thermal and RGB images, and JSON for the annotations. As described earlier, the images have annotations with bounding boxes and the detected items such as person and car. In addition, the reasoning behind not collecting our own data stems from the high-cost of the camera required to take quality thermal images. As shown by Figure 3, the Teledyne FLIR camera produced the images that were used in our training and testing sets.  As mentioned in our approach, we used the YOLOv5 framework to train and evaluate our model.  We developed our own scripts to format the dataset annotations and prepare the data as inputs.  For this project, we evaluated a model that can correctly identify and classify objects of interest from provided IR inputs.  Since the FLIR dataset contains RGB equivalents as well, also trained a model on RGB inputs and compared the two models under a variety of low-visibility conditions. </p>

			<div class="container-fluid text-center" style="padding: 15px;">
				<img src="https://cdn.discordapp.com/attachments/463193265416699915/1047002341808799797/image.png" style="width: 60%">
				<p>Figure 3. Teledyne FLIR Tau 2 Thermal Camera</p>
			</div>

			<p>After developing an approach and method processes to develop our system, the first course of action was to pre-process the data. Although the dataset contained thousands of thermal test images, we sampled the set of images in order to remove corrupted images and allow for our model to train in a reasonable amount of time due to the constraints of our hardware. The sampled set of data was then divided into images taken during the day and images taken at night to develop both a training and validation set. This was done to ensure that IR images are not impacted by the amount of light in the image, rather the IR radiation emitted from different sources to develop the grayscale image.  After processing, the RGB data set contained ~600 daytime images for training and ~400 nighttime images for validation.  The thermal data set contained about ~100 daytime images for training and ~100 nighttime images for validation as well. </p>
			<p>The annotations for each image were in a rough MSCOCO format contained within JSON files [7].  Image width and height, bounding box locations, annotation to image mappings, image to filename mappings, time of day information, and annotation classification mappings were all extracted to filter images based on visibility conditions, aggregate annotations for each image, locate images from their corresponding index, convert top-left bounding box annotations to image center annotations, normalize bounding box annotations, and ascribe proper classification to classification indices.  Text file labels were then produced in the YOLO format for each image. Then, YAML files were created to link the file paths of each validation and training set and provide indexing information on each classification label [6, 8].</p>
			<p>Once formatting and configuration was complete, all data image and label data was uploaded to a shared drive where it could be accessed in Google Colab. The YOLOv5 GitHub repository was forked to a custom repository and the YAML files were added to it so that the changes were easily accessible on Colab [6].  A Jupyter Notebook was created in Google Colab to mount the shared drive, pull the custom YOLOv5 repository, import the necessary libraries, train the RGB and thermal models, and run detections on example images.  The Colab script also contains some rudimentary code to explore the data set configurations and process the data set appropriately, but the bulk of this formatting was done locally using a refined Python script.</p>
			<p>Model training was performed using the default YOLO training configurations, with three epochs for the RGB model and thirty for the thermal model [6].  Since the RGB data set was about an order of magnitude greater than the thermal dataset, training took much longer.  In an ideal environment, both models would be trained on a more similar number of epochs, but since training was performed using the initial weights from the default YOLOv5 model, which are quite good to begin with, each model was able to identify objects of interest.  Since base Google Colab notebooks run for a finite duration before timing out, it was not possible to train the models for longer amounts of time.  In an ideal environment, these models would have been trained on local machines with GPU support for hundreds if not thousands of epochs to derive meaningful quantitative results.  For this reason, the majority of this project's findings will stem from an analysis of the qualitative results.</p>
			<p>In terms of the output of the model running on validation images, the following images in Figures 4 and 5 show the bounding boxes accurately identifying different parts of the IR images. This was run to display identified objects at a confidence threshold of 0.25 or greater.  When this threshold is lowered, we can see that several more objects are identified as shown by Figure 6. Although most of the images were taken from a farther distance, in Figure 7 we can see that there are two people up close and that the model was able to correctly identify them with a high level of confidence. </p>
			<p>Since the RGB model was unable to be as refined as the IR model due to it being run for fewer epochs, the model was still able to identify objects to a reasonable degree as shown by Figure 8. Diving into the statistics developed specifically for the model, Figure 9 also shows different variables (specific forms of accuracy and loss) that were collected by the model through the 30 epochs. Out of these collected values, the most important to us were the mAP@0.5 and mAP@0.5:0.95. These two calculated the mean average precision for the model, which was defined as a ratio of number of correct predictions to total predictions. Mean average prediction at a specific value serves as a threshold to subset the ranked data [9]. Although these values are not as high as we would like, given faster hardware to develop the model over hundreds of epochs, we believe that the precision would have continued to increase and provide us with a stronger model.</p> 

			<div class="container-fluid text-center" style="padding: 15px;">
				<img src="https://cdn.discordapp.com/attachments/463193265416699915/1047002699868155984/image.png" style="width: 60%">
				<p>Figure 4. IR Validation Image</p>
			</div>

			<div class="container-fluid text-center" style="padding: 15px;">
				<img src="https://cdn.discordapp.com/attachments/463193265416699915/1047002809746329610/image.png" style="width: 60%">
				<p>Figure 5. IR Validation Image</p>
			</div>

			<div class="container-fluid text-center" style="padding: 15px;">
				<img src="https://cdn.discordapp.com/attachments/463193265416699915/1047003023714549881/image.png" style="width: 60%">
				<p>Figure 6. IR Validation Image (lower threshold)</p>
			</div>

			<div class="container-fluid text-center" style="padding: 15px;">
				<img src="https://cdn.discordapp.com/attachments/463193265416699915/1047003230204342292/image.png" style="width: 60%">
				<p>Figure 7.IR Image Validation (closer identification)</p>
			</div>

			<div class="container-fluid text-center" style="padding: 15px;">
				<img src="https://cdn.discordapp.com/attachments/463193265416699915/1047003344767549481/image.png" style="width: 60%">
				<p>Figure 8. RGB Validation Image</p>
			</div>

			<div class="container-fluid text-center" style="padding: 15px;">
				<img src="https://cdn.discordapp.com/attachments/463193265416699915/1047003442431922177/image.png" style="width: 60%">
				<p>Figure 9. Output Variable Results over Epochs</p>
			</div>

			<h2> Qualitative Results </h2>

			<p>One of the differences we found when comparing object identification between IR and RGB images is the advantage that IR holds in understanding the scene better than RGB. In a situation where a picture is taken at night on the street roads, several RGB images such as Figure 10 have an abundance of light due to cars’ headlights and the model does not classify them as cars because the lights completely engulf the outline of the vehicle. In the same image, a minivan on the left with the lights off is accurately identified. A comparative IR image such as Figure 11, accurately identifies a car because the actual light in the atmosphere does not impact the development of the IR image. Similarly, Figure 12 fails to identify a car on the left-hand, darker side of the image. If this specific image was taken in IR, it is possible that there would have been a greater chance of the IR object identification system picking up the car. Another example of Figure 13 shows incorrect identification of a traffic light as a stop sign. The RGB image was again extracted from a video at night with poor exposure conditions and the blur of the red light due to the quality of the recording forms a more circular-like shape that we can assume was misclassified as a stop sign. </p>
			<p>Although the IR identification model was more accurate than the RGB model for this given instance, there are still situations where RGB can be advantageous. If the images are taken in a room where there is consistent lighting, but maybe a lot of heat sources close by (such as a conference room where people are meeting), then RGB would be a stronger choice for object identification. However, the practical military and commercial applications of IR to identify objects in areas of low visibility still serve as a great alternative to traditional image collection.</p>

			<div class="container-fluid text-center" style="padding: 15px;">
				<img src="https://cdn.discordapp.com/attachments/463193265416699915/1047003607205150730/image.png" style="width: 60%">
				<p>Figure 10. RGB Validation Image</p>
			</div>

			<div class="container-fluid text-center" style="padding: 15px;">
				<img src="https://cdn.discordapp.com/attachments/463193265416699915/1047003696166350868/image.png" style="width: 60%">
				<p>Figure 11. IR Validation Image</p>
			</div>

			<div class="container-fluid text-center" style="padding: 15px;">
				<img src="https://cdn.discordapp.com/attachments/463193265416699915/1047003814546395196/image.png" style="width: 60%">
				<p>Figure 12. RGB Validation Image</p>
			</div>

			<div class="container-fluid text-center" style="padding: 15px;">
				<img src="https://cdn.discordapp.com/attachments/463193265416699915/1047003898545705081/image.png" style="width: 60%">
				<p>Figure 13. RGB Validation Image</p>
			</div>

			<h2> Conclusion </h2>

			<p>In this project we have presented a model that can accurately detect common objects including people, bikes, cars, trains, and traffic lights in a given IR image, especially in low visibility conditions where RGB image detection might not be possible. Despite not having enough computing power to develop the models to a level that would take full advantage of the architecture of the model and available data set, this goal was met by showing qualitatively and analytically what the strengths of each format was. Through this we saw that IR images worked best when used in low-visibility environments where there may be uneven lighting or obstructive weather conditions. RGB images on the other hand worked better in environments where there is a concentrated amount of heat sources such as crowded areas. Despite this down side, having IR as a choice for image inputs to computer vision tasks is a great tool to be aware of. In the future, these models would be trained with greater computing resources, the full set of training and validation data from the FLIR data set, and a much greater number of epochs.  In addition, data sets that contain matching RGB and IR samples would be explored to investigate the merits of an IR, RGB ensemble model, which could operate well in both high and low visibility conditions.</p>

			<h2> References </h2>

			<ul>
				<li> [1] “FLIR Thermal Dataset for Algorithm Training | FLIR Systems,” www.flir.com. https://www.flir.com/oem/adas/adas-dataset-form/ </li>
				<li> [2] “Thermal Imaging for Safety and Efficiency in Public Transportation,” www.flir.com. https://www.flir.com/discover/traffic/public-transportation/thermal-imaging-for-safety-and-efficiency-in-public-transportation/  </li>
				<li> [3] Beyan, Cigdem. (2010), “OBJECT TRACKING FOR SURVEILLANCE APPLICATIONS KUSING THERMAL AND VISIBLE BAND VIDEO DATA FUSION, “ doi: 10.13140/RG.2.1.4360.1526. </li>
				<li> [4] Ivašić-Kos, Marina and Krišto, Mate and Pobar, Miran. (2019)., “Human Detection in Thermal Imaging Using YOLO,” 20-24, doi: 10.1145/3323933.3324076. </li>
				<li> [5] C. Jiang et al., “Object detection from UAV thermal infrared images and videos using YOLO models,” International Journal of Applied Earth Observation and Geoinformation, vol. 112, p. 102912, Aug. 2022, doi: 10.1016/j.jag.2022.10291</li>
				<li> [6] G. Jocher, “ultralytics/yolov5,” GitHub, Aug. 21, 2020. https://github.com/ultralytics/yolov5 </li>
				<li> [7] “Create COCO Annotations From Scratch,” Immersive Limit. https://www.immersivelimit.com/tutorials/create-coco-annotations-from-scratch </li>
				<li> [8] B. Dipert, “Exploring Data Labeling and the 6 Different Types of Image Annotation,” Edge AI and Vision Alliance, Apr. 21, 2022. https://www.edge-ai-vision.com/2022/04/exploring-data-labeling-and-the-6-different-types-of-image-annotation/ </li>
				<li> [9] “Mean Average Precision (mAP) Explained,” Paperspace Blog, Oct. 16, 2020. https://blog.paperspace.com/mean-average-precision/ </li>
			</ul>
		</div>

    </body>

	<footer class="footer">
		<div class="feet text-muted">
			Ⓒ Alex Downey, Aaron Wadhwa, Kavin Thirukonda
		</div>
	</footer>

</html>
